# ğŸ“„ Resume Job Role Classification using BERT (Transformer)

This project is a **Resume-to-Job Role Classification System** built using a **custom BERT-style Transformer Encoder**.  
It takes resume text as input and predicts the most suitable **job category/role**.

The pipeline includes **PDF resume text extraction**, preprocessing, tokenization, padding, label encoding, and a Transformer-based classification model.

---

## ğŸš€ Features
- Extracts text from resume PDFs using `pdfplumber`
- Tokenizes resumes using `bert-base-uncased` tokenizer
- Custom implementation of:
  - Self-Attention (Q, K, V)
  - Multi-Head Attention
  - Encoder Layers
  - Positional Embeddings
  - `[CLS]` token based classification
- Supports **partial fine-tuning** using pretrained HuggingFace BERT embeddings
- Predicts job role from resume text

---

## ğŸ§  Model Architecture
The model follows the Transformer Encoder design:\

Input Token IDs\
â†“\
Token Embedding + Positional Embedding\
â†“\
Encoder Layer Stack (Multi-Head Attention + FFN)\
â†“\
[CLS] Token Representation\
â†“\
Classification Head\
â†“\
Job Role Prediction\


---


## ğŸ“‚ Dataset Structure
Dataset is organized as folders, where each folder name represents the job role:
``` bash
data/
â”œâ”€â”€ ENGINEERING/
â”œâ”€â”€ FITNESS/
â”œâ”€â”€ ARTS/
â”œâ”€â”€ SALES/
â””â”€â”€ ...
```

Each folder contains multiple resume PDF files.

---

## âš™ï¸ Installation

```bash
pip install pdfplumber tiktoken tqdm transformers torch pandas numpy
```

---

## ğŸ“Œ Tech Stack
- Python  
- PyTorch  
- Transformers (HuggingFace)  
- PDFPlumber  
- NumPy, Pandas  

---

## ğŸ“ˆ Future Improvements
- Add sliding window chunking for resumes longer than 512 tokens  
- Implement attention masking for PAD tokens  
- Compare results with HuggingFace `BertForSequenceClassification`  
- Deploy the model using Streamlit  

---

## ğŸ‘©â€ğŸ’» Author
**Ananya Chattarjee**  
ğŸ“ Jaipur, Rajasthan, India  
ğŸ”— GitHub: [AnanyaChattarjee](https://github.com/AnanyaChattarjee)  
